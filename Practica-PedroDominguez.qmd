---
format: html
editor: visual
  markdown: 
    wrap: 72
---

Vasmos a cargar el dataset de AirBnB descargado de [aquí](https://public.opendatasoft.com/explore/dataset/airbnb-listings/export/?disjunctive.host_verifications&disjunctive.amenities&disjunctive.features&q=Madrid&dataChart=eyJxdWVyaWVzIjpbeyJjaGFydHMiOlt7InR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQ09VTlQiLCJ5QXhpcyI6Imhvc3RfbGlzdGluZ3NfY291bnQiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiJyYW5nZS1jdXN0b20ifV0sInhBeGlzIjoiY2l0eSIsIm1heHBvaW50cyI6IiIsInRpbWVzY2FsZSI6IiIsInNvcnQiOiIiLCJzZXJpZXNCcmVha2Rvd24iOiJyb29tX3R5cGUiLCJjb25maWciOnsiZGF0YXNldCI6ImFpcmJuYi1saXN0aW5ncyIsIm9wdGlvbnMiOnsiZGlzanVuY3RpdmUuaG9zdF92ZXJpZmljYXRpb25zIjp0cnVlLCJkaXNqdW5jdGl2ZS5hbWVuaXRpZXMiOnRydWUsImRpc2p1bmN0aXZlLmZlYXR1cmVzIjp0cnVlfX19XSwidGltZXNjYWxlIjoiIiwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D&location=16,41.38377,2.15774&basemap=jawg.streets)

![](descargar.png)

```{r}
airbnb<-read.csv('airbnb-listings.csv',sep = ';')
options(repr.plot.height=4,repr.plot.width=6,repr.plot.res = 300)
head(airbnb)
```

1.  Vamos a quedarnos con las columnas de mayor interés: 'City','Room.Type','Neighbourhood','Accommodates','Bathrooms','Bedrooms','Beds','Price','Square.Feet','Guests.Included','Extra.People','Review.Scores.Rating','Latitude', 'Longitude' Nos quedarmos solo con las entradas de Madrid para Room.Type=="Entire home/apt" y cuyo barrio (Neighbourhood) no está vacio '' Podemos eliminar las siguientes columnas que ya no son necesarias: "Room.Type",'City' Llama a nuevo dataframe df_madrid.

```{r}
#selecionamos columnas
df_madrid <- airbnb[, c('City', 'Room.Type', 'Neighbourhood', 'Accommodates', 'Bathrooms', 'Bedrooms', 'Beds', 'Price', 'Square.Feet', 'Guests.Included', 'Extra.People', 'Review.Scores.Rating', 'Latitude', 'Longitude')]

#filtramos por ciudad y por tipo de habitacion
df_madrid <- df_madrid[df_madrid$City == "Madrid" & df_madrid$Room.Type == "Entire home/apt", ]

#eliminamos barrio vacio
df_madrid <- df_madrid[df_madrid$Neighbourhood != "", ]

#eliminamos ciudad y tipo de habitacion
df_madrid <- df_madrid[, c('Neighbourhood', 'Accommodates', 'Bathrooms', 'Bedrooms', 'Beds', 'Price', 'Square.Feet',
                           'Guests.Included', 'Extra.People', 'Review.Scores.Rating', 'Latitude', 'Longitude')]

```

------------------------------------------------------------------------

2.  Crea una nueva columna llamada Square.Meters a partir de Square.Feet. Recuerda que un pie cuadrado son 0.092903 metros cuadrados.

```{r}

df_madrid$Square.Meters <- df_madrid$Square.Feet * 0.092903
  

```

------------------------------------------------------------------------

3.  ¿Que porcentaje de los apartamentos no muestran los metros cuadrados? Es decir, ¿cuantos tienen NA en Square.Meters?

```{r}

porcentaje_na <- sum(is.na(df_madrid$Square.Meters)) / nrow(df_madrid) * 100

porcentaje_na
  
```

------------------------------------------------------------------------

4.  De todos los apartamentos que tienen un valor de metros cuadrados diferente de NA ¿Que porcentaje de los apartamentos tienen 0 metros cuadrados?

```{r}
#primero filtramos los apartamentos que no son NA
df_madrid_sin_NA <- df_madrid[!is.na(df_madrid$Square.Meters), ]

#Calculamos el porcentaje de los apartamentos que tienen 0 metros cuadrados respecto a 
#aquellos que no son NA
porcentaje_0 <- sum(df_madrid_sin_NA$Square.Meters == 0) / nrow(df_madrid_sin_NA) * 100

porcentaje_0
  
```

------------------------------------------------------------------------

5.  Reemplazar todos los 0m\^2 por NA

```{r}

df_madrid$Square.Meters[df_madrid$Square.Meters == 0] <- NA
  
```

------------------------------------------------------------------------

Hay muchos NAs, vamos a intentar crear un modelo que nos prediga cuantos son los metros cuadrados en función del resto de variables para tratar de rellenar esos NA. Pero **antes de crear el modelo** vamos a hacer: \* pintar el histograma de los metros cuadrados y ver si tenemos que filtrar algún elemento más. \* crear una variable sintética nueva basada en la similitud entre barrios que usaremos en nuestro modelo.

6.  Pinta el histograma de los metros cuadrados y ver si tenemos que filtrar algún elemento más

```{r}

hist(df_madrid$Square.Meters, main = "Histograma de Metros Cuadrados", xlab = "m^2")
  
```

```{r}

#vamos a fijarnos mas en el rango de 0 a 100

hist(df_madrid$Square.Meters, main = "Histograma de Metros Cuadrados", xlab = "m^2", xlim = c(0,100), breaks = 50)
  
```

PD: Se observa que hay mmuchos apartamentos con menos de 10 m2

7.  Asigna el valor NA a la columna Square.Meters de los apartamentos que tengan menos de 20 m\^2

```{r}

df_madrid$Square.Meters[df_madrid$Square.Meters < 20] <- NA
```

------------------------------------------------------------------------

8.  Existen varios Barrios que todas sus entradas de Square.Meters son NA, vamos a eliminar del dataset todos los pisos que pertenecen a estos barrios.

```{r}

#Sumamos las entradas de cada barrio
Num_entradas_barrios <- table(droplevels(df_madrid['Neighbourhood']))
Num_entradas_barrios
```

```{r}
#filtramos las entradas NA
df_madrid_NA <- df_madrid[is.na(df_madrid$Square.Meters), ]

#sumamos las entradas NA de cada barrio
Num_entradas_barrios_NA <- table(droplevels(df_madrid_NA['Neighbourhood']))
Num_entradas_barrios_NA
```

```{r}
#listado de barrios
barrios <- names(Num_entradas_barrios_NA)

#inicializamos el listado de barrios con todo NA
barrios_NA <- character()

for (i in 1:length(barrios)) {
  if (Num_entradas_barrios[barrios[i]] == Num_entradas_barrios_NA[barrios[i]]) {
    barrios_NA <- c(barrios_NA, barrios[i])
  }
}

df_madrid <- df_madrid[!(df_madrid$Neighbourhood %in% barrios_NA), ]

```

------------------------------------------------------------------------

El barrio parece ser un indicador importante para los metros cuadrados de un apartamento.

Vamos a agrupar los barrios por metros cuadrados. Podemos usar una matriz de similaridad de Tukey tal y como hicimos en el curso de estadística:

```{r}
tky<-TukeyHSD(aov( formula=Square.Meters~Neighbourhood, data=df_madrid ))
tky.result<-data.frame(tky$Neighbourhood)
cn <-sort(unique(df_madrid$Neighbourhood))
resm <- matrix(NA, length(cn),length(cn))
rownames(resm) <- cn
colnames(resm) <- cn
resm[lower.tri(resm) ] <- round(tky.result$p.adj,4)
resm[upper.tri(resm) ] <- t(resm)[upper.tri(resm)] 
diag(resm) <- 1
library(ggplot2)
library(reshape2)
dfResm <- melt(resm)
ggplot(dfResm, aes(x=Var1, y=Var2, fill=value))+
  geom_tile(colour = "black")+
  scale_fill_gradient(low = "white",high = "steelblue")+
  ylab("Class")+xlab("Class")+theme_bw()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1),legend.position="none")
```

9.  Usando como variable de distancia: 1-resm Dibuja un dendrograma de los diferentes barrios.

```{r}
library(dendextend)
library(cluster) 
dist_resm <- as.dist(1-resm)
dist_resm.tree <- hclust(dist_resm, method = "complete")
dist_resm.dend <- as.dendrogram(dist_resm.tree)
par(cex =0.3)
plot(dist_resm.dend)
```

------------------------------------------------------------------------

10. ¿Que punto de corte sería el aconsejable?, ¿cuantos clusters aparecen?

PD: en el anterior apartado se oobserva que el numero de cluster estara entre 2, 3 o 4 cluster, vamos a dijar el silhouette para ver la calidad

```{r}

ss <- silhouette(cutree(dist_resm.tree, k = 2), dist_resm)
plot(ss,col=1:max(cutree(dist_resm.tree, k = 2)),border=NA)


```

```{r}

ss <- silhouette(cutree(dist_resm.tree, k = 3), dist_resm)
plot(ss,col=1:max(cutree(dist_resm.tree, k = 3)),border=NA)


```

```{r}

ss <- silhouette(cutree(dist_resm.tree, k = 4), dist_resm)
plot(ss,col=1:max(cutree(dist_resm.tree, k = 4)),border=NA)

```

PD: Como se observa para 2 y 3 cluster la calidad es muy parecida(0.92 y 0.91) pero cuando pasamos a 4 cluster la calidad baja drasticamete hasta 0.24. Por lo que la calidad optima estaria en 3 clusteres

11. Vamos a crear una nueva columna en el dataframe df_madrid con un nuevo identificador marcado por los clusters obtenidos. Esta columna la llamaremos neighb_id

```{r}

clusters <-  cutree(dist_resm.dend, k=3)


neighbourhood_cluster <- data.frame(Neighbourhood = names(clusters),
                              neighb_id = clusters)

df_madrid <-merge(df_madrid, neighbourhood_cluster, by = "Neighbourhood")



```

------------------------------------------------------------------------

12. Vamos a crear dos grupos, uno test y otro train.

```{r}

set.seed(12345)
idx<-sample(1:nrow(df_madrid),nrow(df_madrid)*0.7)
df_madrid.train<-df_madrid[idx,]
df_madrid.test <-df_madrid[-idx,]



```

------------------------------------------------------------------------

13. Tratamos de predecir los metros cuadrados en función del resto de columnas del dataframe.

```{r}

df_madrid.train <- df_madrid.train[!is.na(df_madrid.train$Square.Meters), ]
df_madrid.test <- df_madrid.test[!is.na(df_madrid.test$Square.Meters), ]

```


```{r}
library(GGally)
library(dplyr)

ggpairs(df_madrid  |> select(Accommodates,Bathrooms,Bedrooms,Beds,Price,neighb_id,Guests.Included,Review.Scores.Rating,Square.Meters), 
        #lower = list(continuous = wrap("density", alpha = 0.8,size=0.2,color='blue'))
       lower = list(continuous = wrap("points", alpha = 0.3,size=0.1,color='blue'))
       ) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

PD: A simple vista las variables Guests.Included y Review.Scores no vamos a tenerlas en cuenta ya que su correlacion es muy baja

```{r}

modelo <- lm(Square.Meters ~ Accommodates + Bathrooms + Bedrooms + Beds + Price + neighb_id, data = df_madrid.train)

summary(modelo)

```

------------------------------------------------------------------------

14. Mirad el histograma de los residuos sobre el conjunto de test para evaluar la calidad de vuestro modelo

```{r}


predictions <- predict(modelo, newdata = df_madrid.test)
caret::postResample(predictions, df_madrid.test$Square.Meters)
```

```{r}
hist(df_madrid.test$Square.Meters-predictions,xlim = c(-100,100), breaks = 50)

```

PD: Se observa un Rsquared de 0.72 y aproximadamente se observa una distribucion normal del error, por lo el modelo se puede interpretar como aceptablemente bueno 

------------------------------------------------------------------------

15. Si tuvieramos un anuncio de un apartamento para 6 personas (Accommodates), con 1 baño, con un precio de 80€/noche y 3 habitaciones en el barrio de Sol, con 3 camas y un review de 80. ¿Cuantos metros cuadrados tendría? Si tu modelo necesita algúna variable adicional puedes inventartela dentro del rango de valores del dataset. ¿Como varía sus metros cuadrados con cada habitación adicional?


```{r}
consulta <- data.frame(Accommodates=6,Bathrooms=1,Bedrooms=3,Beds=3,Price=80,neighb_id=2)
```

```{r}
respuesta <- predict(modelo, newdata = consulta)
respuesta
```

PD: Tendria 96.92 m2 y por cada habitacion adicional se aumentarian 16.66m2

------------------------------------------------------------------------

16. Rellenar los Square.Meters con valor NA con el estimado con el modelo anterior.

```{r}
datos_faltantes <- df_madrid[is.na(df_madrid$Square.Meters), ]
```

```{r}
predicciones_faltantes <-predict(modelo, newdata = datos_faltantes)
```

```{r}
df_madrid$Square.Meters[is.na(df_madrid$Square.Meters)] <- predicciones_faltantes
```

------------------------------------------------------------------------

17. Usar PCA para encontrar el apartamento más cercano a uno dado. Este algoritmo nos ayudaría a dado un apartamento que el algoritmo nos devolvería los 5 apartamentos más similares.

Crearemos una función tal que le pasemos un apartamento con los siguientes datos: \* Accommodates \* Bathrooms \* Bedrooms \* Beds \* Price \* Guests.Included \* Extra.People \* Review.Scores.Rating \* Latitude \* Longitude \* Square.Meters

y nos devuelva los 5 más similares de:

```{r}
#eliminamos las columnas square feet
df_madrid <- df_madrid[, c('Accommodates', 'Bathrooms', 'Bedrooms', 'Beds', 'Price', 'Square.Meters','Guests.Included', 'Extra.People', 'Review.Scores.Rating', 'Latitude', 'Longitude','neighb_id')]
```

```{r}
#eliminamos NA
df_madrid <- df_madrid[!is.na(df_madrid$Accommodates), ]
df_madrid <- df_madrid[!is.na(df_madrid$Bathrooms), ]
df_madrid <- df_madrid[!is.na(df_madrid$Bedrooms), ]
df_madrid <- df_madrid[!is.na(df_madrid$Beds), ]
df_madrid <- df_madrid[!is.na(df_madrid$Price), ]
df_madrid <- df_madrid[!is.na(df_madrid$Square.Meters), ]
df_madrid <- df_madrid[!is.na(df_madrid$Guests.Included), ]
df_madrid <- df_madrid[!is.na(df_madrid$Extra.People), ]
df_madrid <- df_madrid[!is.na(df_madrid$Review.Scores.Rating), ]
df_madrid <- df_madrid[!is.na(df_madrid$Latitude), ]
df_madrid <- df_madrid[!is.na(df_madrid$Longitude), ]
df_madrid <- df_madrid[!is.na(df_madrid$neighb_id), ]
```



```{r}
#Utilizamos prcomp para calcular el pca
prmadrid<-prcomp(df_madrid,center = TRUE, scale = TRUE)
#Comprobamos que los siete primeros autovalores contienen aproximadamente el 90% de la varianza
plot(prmadrid$sdev^2/sum(prmadrid$sdev^2),main="Autovalores")
#esto lo dijiste en la clase pero no lo voy a utilizar
```
```{r}

#El piso objetivo lo vamos a elegir que sea casi igual al primer piso que aparece en el dataframe
#con el obetivo d ver si fnciiona el algoritmo
piso_objetivo <- data.frame(
  Accommodates = 6,
  Bathrooms = 1,
  Bedrooms = 3,
  Beds = 3,
  Price = 50,
  Square.Meters = 68,
  Guests.Included = 2,
  Extra.People = 10,
  Review.Scores.Rating = 68,
  Latitude = 40.40226,
  Longitude = -3.712753,
  neighb_id = 1
)


pred_piso_objetivo <- predict(prmadrid, newdata = piso_objetivo)

```

```{r}

resultados<- data.frame(rownames(df_madrid),distancia=NA)

for (id_test_piso in 1:nrow(df_madrid)){

  piso_real<-df_madrid[id_test_piso,]

  pred_piso_real<-predict(prmadrid, newdata = piso_real)
        
  resultados$distancia[id_test_piso]<-rowSums((pred_piso_real-pred_piso_objetivo)^2)    
    
}
```

```{r}
resultados_ordenados <- resultados[order(resultados$distancia), ]
primeros_5 <- head(resultados_ordenados, 5)
#Imprimimos los 5 primeros que seran los mas cercanos
print(primeros_5)

```

------------------------------------------------------------------------
